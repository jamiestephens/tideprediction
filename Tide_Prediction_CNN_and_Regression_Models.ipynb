{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jstephens/tideprediction/blob/main/Tide_Prediction_CNN_and_Regression_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bx28FbEqpxLC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import datetime\n",
        "import joblib\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.neighbors import KDTree\n",
        "from scipy.spatial.distance import euclidean\n",
        "from tensorflow.python.framework.errors_impl import ResourceExhaustedError\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose,Lambda, Dropout, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.cluster import KMeans\n",
        "import random\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw09Md1q51SH",
        "outputId": "3286922d-dfa4-49b2-8ddc-1754709163a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dir = 'drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnXhOF--q7z0"
      },
      "source": [
        "# Data Load and Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cmOSCzoipymK"
      },
      "outputs": [],
      "source": [
        "X_train = np.load(base_dir + 'source/X_train_surge_new.npz')\n",
        "X_test = np.load(base_dir +'source/X_test_surge_new.npz')\n",
        "Y_train = pd.read_csv(base_dir +'/source/Y_train_surge.csv',index_col = 'id_sequence')\n",
        "\n",
        "df_X_test = pd.DataFrame.from_dict({item: X_test[item] for item in X_test.files}, orient='index').T.set_index('id_sequence')\n",
        "df_X_train = pd.DataFrame.from_dict({item: X_train[item] for item in X_train.files}, orient='index').T.set_index('id_sequence')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E3pulYDWoWUC"
      },
      "outputs": [],
      "source": [
        "def normalization(df):\n",
        "  for i in range(len(df['slp'])):\n",
        "      # Access the inner arrays\n",
        "      inner_arrays = df['slp'].iloc[i]\n",
        "\n",
        "      # Normalize each inner array\n",
        "      normalized_inner_arrays = [normalize(arr) for arr in inner_arrays]\n",
        "\n",
        "      # Update the DataFrame with the normalized inner arrays\n",
        "      df['slp'].iloc[i] = normalized_inner_arrays\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Aen07SbWp6W3"
      },
      "outputs": [],
      "source": [
        "X_test = normalization(df_X_test)\n",
        "X_train = normalization(df_X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N_6JzB47qHJW"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_sequences(df, use_shortened_data=True):\n",
        "    new_data = df['slp'].apply(lambda x: x[::3])   # select every third image in every row\n",
        "    seqcount = 13\n",
        "    new_df = pd.DataFrame(new_data, columns=['slp']) \n",
        "\n",
        "    if use_shortened_data:\n",
        "        image_sequences = np.array(new_df['slp'].values.tolist())   \n",
        "    else:\n",
        "        image_sequences = np.array(new_df['slp'].values.tolist())\n",
        "    image_sequences = image_sequences[:, -seqcount:]\n",
        "    reshaped_sequences = image_sequences.reshape(-1, seqcount, 41, 41, 1)\n",
        "    return reshaped_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_seq_train = load_and_preprocess_sequences(X_train, use_shortened_data=True)\n",
        "reshaped_seq_test = load_and_preprocess_sequences(X_test, use_shortened_data=True)"
      ],
      "metadata": {
        "id": "R8eqA8JQpcHZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_array(row):\n",
        "    new_row = []\n",
        "    for col in row:\n",
        "        if isinstance(col, np.ndarray):\n",
        "            new_row.extend(col.ravel())\n",
        "        else:\n",
        "            new_row.append(col)\n",
        "    return pd.Series(new_row)\n",
        "\n",
        "def X_prep(df):\n",
        "  included_columns = ['surge1_input','surge2_input','t_surge1_output','t_surge2_output']\n",
        "\n",
        "  df = df[included_columns]\n",
        "  expandedcol_list = []\n",
        "  count = 0\n",
        "  for x in included_columns:\n",
        "    for count in range(10):\n",
        "      expandedcol_list.append(f\"{x}_{count}\")\n",
        "\n",
        "  df = df.apply(flatten_array, axis=1)\n",
        "  df.columns = range(1, len(df.columns) + 1)\n",
        "  df.columns = expandedcol_list\n",
        "  return df"
      ],
      "metadata": {
        "id": "L06-jJiw6Aje"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_prep(X_train)\n",
        "X_test = X_prep(X_test)"
      ],
      "metadata": {
        "id": "62F2OXzF4kvJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY7PPhyspFvx"
      },
      "source": [
        "# GPU Maintenance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o9Jo_21nlibS"
      },
      "outputs": [],
      "source": [
        "# Check if a GPU is available\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    # Configure TensorFlow to use the first GPU\n",
        "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "09X3x8p4paSF"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Modeling"
      ],
      "metadata": {
        "id": "Siz-HViV3Nih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_model(cluster_train_labels, cluster_test_labels,clustercount,iteration):\n",
        "  num_rows = len(X_test)\n",
        "\n",
        "  X_train['Cluster'] = cluster_train_labels\n",
        "  X_test['Cluster'] = cluster_test_labels\n",
        "  cluster_scores = []\n",
        "\n",
        "  data = np.zeros((num_rows, 20))  # initialize with zeros\n",
        "  entire_y_pred_df = pd.DataFrame(data)\n",
        "  entire_y_pred_df = entire_y_pred_df.set_index(X_test.index)\n",
        "\n",
        "  for cluster_no in range(clustercount):\n",
        "    c_X_train = X_train[X_train['Cluster'] == cluster_no].copy()\n",
        "    c_X_test = X_test[X_test['Cluster'] == cluster_no].copy()\n",
        "    c_y_train = Y_train.loc[c_X_train.index]\n",
        "\n",
        "    # Create a multi-output linear regression model\n",
        "    model = MultiOutputRegressor(LinearRegression())\n",
        "\n",
        "    # Perform cross-validation\n",
        "    scores = cross_val_score(model, c_X_train, c_y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "        \n",
        "    # building out prediction file\n",
        "    model.fit(c_X_train, c_y_train)\n",
        "    y_pred = model.predict(c_X_test)\n",
        "    y_pred_df = pd.DataFrame(y_pred)\n",
        "    y_pred_df = y_pred_df.set_index(c_X_test.index)\n",
        "\n",
        "    entire_y_pred_df.update(y_pred_df)\n",
        "    # Calculate the mean squared error\n",
        "    mean_mse = -scores.mean()\n",
        "    cluster_scores.append(mean_mse)\n",
        "\n",
        "  entire_y_pred_df.columns=Y_train.columns\n",
        "  predictions_file = f\"{base_dir}Predictions/{timestamp}_{iteration}_y_predictions_{cluster_no}_clusters.csv\"\n",
        "\n",
        "  entire_y_pred_df.to_csv(predictions_file)\n",
        "  print(f\"Iteration {iteration}: predictions for {cluster_no} clusters exported successfully.\")\n",
        "  avg_mse = np.mean(cluster_scores)\n",
        "  median_mse = np.median(cluster_scores)\n",
        "  mse_scores= [avg_mse, median_mse]\n",
        "\n",
        "  return mse_scores"
      ],
      "metadata": {
        "id": "Tra1IDt85jcE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD55pKskwQ_8"
      },
      "source": [
        "# Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "export_grid_search_results collects all the relevant statistics for each parameter outcome, then by cluster, and exports it to CSV. Unsuccessful iterations - where the clustering couldn't be applied properly or there wasn't enough memory to test the set of parameters - are listed separately."
      ],
      "metadata": {
        "id": "yfWX4BxLy7i9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RBvutW2Tuqw-"
      },
      "outputs": [],
      "source": [
        "def export_grid_search_results(grid_search_df,unsuccessful_iterations,oom_list):\n",
        "    summary_file_path = f'{base_dir}{timestamp}_summary_stats.csv'\n",
        "\n",
        "    grid_search_df.to_csv(summary_file_path, index=False, sep=',')\n",
        "    \n",
        "    output = '\\n\\n\\n\\n'\n",
        "    if len(unsuccessful_iterations) >= 1:\n",
        "      output += '\\nUnsuccessful Parameter Combinations:'\n",
        "      for unsuccessful_item in unsuccessful_iterations:\n",
        "        output += f'\\n{unsuccessful_item}'\n",
        "    \n",
        "    if len(oom_list) >= 1:\n",
        "      output += '\\nInsufficient Memory:'\n",
        "      for oom_item in oom_list:\n",
        "        output += f'\\n{oom_item}'\n",
        "\n",
        "    with open(summary_file_path, \"a\") as file:\n",
        "      file.write(output)\n",
        "    \n",
        "    print(\"Grid search summary successfully exported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5sbCqMv3qImV"
      },
      "outputs": [],
      "source": [
        "def create_autoencoder_model(latent_dim, conv1_filters, conv2_filters, dropout_rate, l_r):\n",
        "    input_shape = (13, 41, 41, 1)\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = TimeDistributed(Conv2D(conv1_filters, 3, activation='softmax', padding='same'))(inputs)\n",
        "    x = TimeDistributed(Conv2D(conv2_filters, 3, activation='softmax', padding='same'))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "    x = TimeDistributed(Dropout(dropout_rate))(x)\n",
        "    latent = TimeDistributed(Dense(latent_dim, activation='softmax'))(x)\n",
        "    x = TimeDistributed(Dense(np.prod((41, 41, 64)), activation='softmax'))(latent)\n",
        "    x = TimeDistributed(Reshape((41, 41, 64)))(x)\n",
        "    x = TimeDistributed(Conv2DTranspose(32, 3, activation='softmax', padding='same'))(x)\n",
        "    outputs = TimeDistributed(Conv2DTranspose(1, 3, activation='sigmoid', padding='same'))(x)\n",
        "    autoencoder = Model(inputs, outputs)\n",
        "    optimizer = Adam(learning_rate=l_r)\n",
        "    autoencoder.compile(optimizer=optimizer, loss='mse')\n",
        "    return autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2kXyTrcaqJtD"
      },
      "outputs": [],
      "source": [
        "def perform_grid_search(param_grid, num_iterations,use_lin_reg):\n",
        "    \n",
        "    exportdf_columns = ['Iteration No','Latent Dimensions','Batch Size','Epochs',\n",
        "               'Conv1 Filters','Conv2 Filters','Dropout Rate','Learning Rate',\n",
        "               'Clusters','Silhouette Score - Train','Calinski Score - Train',\n",
        "               'Davies Bouldin Score - Train','Avg MSE','Median MSE',\n",
        "               'Silhouette Score - Test','Calinski Score - Test','Davies Bouldin Score - Test']\n",
        "\n",
        "    grid_search_results = pd.DataFrame(columns=exportdf_columns)\n",
        "    unsuccessful_iterations = []\n",
        "    oom_list = []\n",
        "    for iteration_no in range(num_iterations):\n",
        "        print(f\"Iteration: {iteration_no}\")\n",
        "        try:\n",
        "            params = {\n",
        "                'latent_dim': random.choice(param_grid['latent_dim']),\n",
        "                'batch_size': random.choice(param_grid['batch_size']),\n",
        "                'epochs': random.choice(param_grid['epochs']),\n",
        "                'conv1_filters': random.choice(param_grid['conv1_filters']),\n",
        "                'conv2_filters': random.choice(param_grid['conv2_filters']),\n",
        "                'dropout_rate': random.choice(param_grid['dropout_rate']),\n",
        "                'learning_rate': random.choice(param_grid['learning_rate'])\n",
        "            }\n",
        "\n",
        "            autoencoder_model = create_autoencoder_model(params['latent_dim'], params['conv1_filters'],\n",
        "                                                         params['conv2_filters'], params['dropout_rate'],\n",
        "                                                         params['learning_rate'])\n",
        "\n",
        "            early_stopping = EarlyStopping(monitor='loss', patience=3)\n",
        "            autoencoder_model.fit(reshaped_seq_train, reshaped_seq_train, epochs=params['epochs'],\n",
        "                                  batch_size=params['batch_size'], callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "            encoder = Model(autoencoder_model.input, autoencoder_model.layers[-5].output)\n",
        "            latent_space = encoder.predict(reshaped_seq_train)\n",
        "            reshaped_latent_space_train = latent_space.reshape(-1, 13 * params['latent_dim'])\n",
        "\n",
        "            for cluster_no in range(2, 6):\n",
        "                kmeans = KMeans(n_clusters=cluster_no, random_state=42, n_init='auto')\n",
        "                cluster_labels_train = kmeans.fit_predict(reshaped_latent_space_train)\n",
        "                try:\n",
        "                    if use_lin_reg == True and len(set(cluster_labels_train)) == cluster_no:\n",
        "                        latent_space_test = encoder.predict(reshaped_seq_test)\n",
        "                        reshaped_latent_space_test = latent_space_test.reshape(-1, 13 * params['latent_dim'])\n",
        "                        cluster_labels_test = kmeans.fit_predict(reshaped_latent_space_test)\n",
        "                        mse_scores = regression_model(cluster_labels_train,cluster_labels_test,cluster_no,iteration_no)\n",
        "\n",
        "                    silhouette_avg = silhouette_score(reshaped_latent_space_train, cluster_labels_train)\n",
        "                    ch_score = calinski_harabasz_score(reshaped_latent_space_train, cluster_labels_train)\n",
        "                    db_score = davies_bouldin_score(reshaped_latent_space_train, cluster_labels_train)\n",
        "\n",
        "                    silhouette_avg_test = silhouette_score(reshaped_latent_space_test, cluster_labels_test)\n",
        "                    ch_score_test = calinski_harabasz_score(reshaped_latent_space_test, cluster_labels_test)\n",
        "                    db_score_test = davies_bouldin_score(reshaped_latent_space_test, cluster_labels_test)\n",
        "\n",
        "                    results_list = [iteration_no,params['latent_dim'],params['batch_size'],params['epochs'],\n",
        "                                       params['conv1_filters'],params['conv2_filters'],params['dropout_rate'],\n",
        "                                       params['learning_rate'],cluster_no,silhouette_avg,ch_score,db_score,\n",
        "                                       mse_scores[0],mse_scores[1],silhouette_avg_test,ch_score_test,\n",
        "                                       db_score_test]\n",
        "                    \n",
        "                    clustering_df = pd.DataFrame([results_list], columns=grid_search_results.columns)\n",
        "\n",
        "                    grid_search_results = grid_search_results.append(clustering_df, ignore_index=True)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "                    unsuccessful_iterations.append([params,e])\n",
        "\n",
        "        except ResourceExhaustedError:\n",
        "            oom_list.append(params)\n",
        "            print(f\"Resource Exhausted Error for {params}\")\n",
        "            continue\n",
        "\n",
        "    export_grid_search_results(grid_search_results,unsuccessful_iterations,oom_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSTe2SpjqWgl",
        "outputId": "d5530a5e-2659-4702-8197-274b772fb2e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0\n",
            "Epoch 1/20\n",
            "Resource Exhausted Error for {'latent_dim': 175, 'batch_size': 128, 'epochs': 20, 'conv1_filters': 150, 'conv2_filters': 150, 'dropout_rate': 0.6, 'learning_rate': 0.0001}\n",
            "Grid search summary successfully exported.\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Create the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'latent_dim': [64, 128, 175],\n",
        "    'batch_size': [32, 64, 128,175],\n",
        "    'epochs': [10, 20, 30],\n",
        "    'conv1_filters': [100, 150, 175],\n",
        "    'conv2_filters': [100, 150, 175],\n",
        "    'dropout_rate': [0.2, 0.4, 0.6],\n",
        "    'learning_rate': [0.01, 0.001, 0.0001],\n",
        "}\n",
        "\n",
        "num_iterations = 1\n",
        "timestamp = datetime.datetime.now()\n",
        "timestamp = timestamp.strftime(\"%Y_%m_%d_%H%M%S\")\n",
        "\n",
        "perform_grid_search(param_grid, num_iterations,True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOig0XTET2U+/TifcLdocoG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}